<!DOCTYPE group PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<group>

<p>This short tutorial shows how to
compute <b><a href="%dox:fisher;">Fisher vector</a></b> and
<b><a href="%dox:vlad;">VLAD</a></b> encodings with VLFeat MATLAB
interface.</p>

<p>These encoding serve a similar purposes: summarising in a vectorial
statistic a number of local feature descriptors
(e.g. <a href="%dox:sift;">SIFT</a>). Similarly to bag of visual
words, they compare local descriptor to a dictionary, obtained with
vectonr quantization (KMeans) in the case of VLAD
and <a href="%dox:gmm;">Gaussina Mixture Models</a> for Fisher
Vectors. However, rather than storing visual word occurences only,
these representation store the difference between dictonary elements
and pooled local features.</p>

<ul>
 <li><a href="%pathto:tut.encoding.fisher;">Fisher encoding</a></li>
 <li><a href="%pathto:tut.encoding.vlad;">VLAD encoding</a></li>
</ul>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.encoding.fisher">Fisher encoding</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>The Fisher encoding uses GMM to construct a visual word
dicionary. To exemplify constructing a GMM, we consider a number of 2
dimensional data points (see also the <a href="%pathto:tut.gmm;">GMM
tutorial</a>). In practice, these would be a collection of SIFT or
other local image features:</p>

<precode type='matlab'>
numFeatures = 5000 ;
dimension = 2 ;
data = rand(dimension,numFeatures) ;

numClusters = 30 ;
[means, sigmas, weights] = vl_gmm(data, numClusters);
</precode>

<p>Next we create another random set of vectors, which should be
encoded using the Fisher vector representation and the GMM just
obtained.</p>

<precode type='matlab'>
numEncodedFeatures = 1000;
dataToBeEncoded = rand(dimension,numEncodedFeatures);
</precode>

<p>The Fisher vector encoding <code>enc</code> of these vectors is
obtained by calling the <code>vl_fisher</code> function using the
output of the <code>vl_gmm</code> function:</p>

<precode type='matlab'>
encoding = vl_fisher(datatoBeEncoded, means, sigmas, weights);
</precode>

<p>The <code>encoding</code> vector is the Fisher vector
representation of the data <code>dataToBeEncoded</code>.</p>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.encoding.vlad">VLAD encoding</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>
The <b>V</b>ector of <b>L</b>inearly <b>A</b>gregated <b>D</b>escriptors
encodes the features in a slightly different way. Simillar to 
the Fisher encoding, the VLAD encoding collaborates with
a clustering technique. In the case of VLAD, this method is 
KMeans clustering.
</p>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h3 id="tut.encoding.vlad.kmeans">KMeans + VLAD</h3>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>
Let's first, as we did in the Fisher section,
make a random dataset <code>dataLearn</code> and cluster
it using KMeans (<a href="%pathto:root;overview/kmeans.html">KMeans</a> tutorial).
Also we will need the dataset we want to encode, so we create one.
</p>

<precode>
N         = 5000 ;
dimension = 2 ;
dataLearn = rand(dimension,N) ;

numClusters = 30 ;
centers = vl_kmeans(dataLearn, numClusters);

Nencode = 1000;
dataEncode = rand(dimension,Nencode);
</precode>

<p>
The <code>vl_vlad</code> function accepts centers of clusters,
data we want to encode and also assignments (which are "hard" in the case of KMeans)  
of each vector to a cluster. The assignments could be obtained by the
vl_kdtree_query function, which quickly finds the nearest cluster center
(stored in <code>centers</code>) for each <code>dataEncode</code> vector.
Note that before running queries to KD-Tree it must be built
using the <code>vl_kdtreebuild</code> function.
</p>

<precode>
kd_tree = vl_kdtreebuild(centers) ;
assign = vl_kdtreequery(kd_tree, centers, dataEncode) ;
</precode>

<p>
Now we have in the <code>assign</code> variable indices of nearest centers
to each <code>dataEncode</code> vector. The next step is 
converting assign vector to the right format, which
is accepted by <code>vl_vlad</code>.
</p>

<precode>
assignments = zeros(numClusters,Nencode);
assignments(sub2ind(size(assignments),assign,1:length(assign))) = 1;
</precode>

<p>
After this, we are ready to proceed to calculate
the final VLAD vector <code>enc</code>.
</p>

<precode>
enc = vl_vlad(dataEncode,centers,assignments);
</precode>

</group>




